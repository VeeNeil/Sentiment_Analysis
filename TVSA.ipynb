{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-14T01:22:26.995115Z",
     "end_time": "2023-09-14T01:22:27.068110Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import  AlbertConfig, AlbertModel\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AlbertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-14T01:22:27.011113Z",
     "end_time": "2023-09-14T01:22:27.232112Z"
    }
   },
   "outputs": [],
   "source": [
    "Dataset_file = \"TV.csv\"\n",
    "df = pd.read_csv(Dataset_file)\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['Cảm xúc'])\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-14T01:22:27.237112Z",
     "end_time": "2023-09-14T01:22:28.244111Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text, add_special_tokens=True, max_length=128, padding='max_length', truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-14T01:22:28.289111Z",
     "end_time": "2023-09-14T01:22:32.240421Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['input_ids'] = train_df['Nhận xét đánh giá'].apply(tokenize_text)\n",
    "val_df['input_ids'] = val_df['Nhận xét đánh giá'].apply(tokenize_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-14T01:31:22.754673Z",
     "end_time": "2023-09-14T01:31:22.769674Z"
    }
   },
   "outputs": [],
   "source": [
    "albert_config = AlbertConfig(\n",
    "    vocab_size=30000,            # Size of the vocabulary\n",
    "    hidden_size=512,            # Size of the hidden layers\n",
    "    num_hidden_layers=10,        # Number of hidden layers\n",
    "    num_attention_heads=10,      # Number of attention heads\n",
    "    intermediate_size=1024,     # Size of the intermediate (feed-forward) layers\n",
    "    hidden_dropout_prob=0.2,    # Dropout probability for hidden layers\n",
    "    attention_probs_dropout_prob=0.2,  # Dropout probability for attention scores\n",
    "    max_position_embeddings=128,  # Maximum position embeddings (adjust based on your sequence length)\n",
    "    type_vocab_size=2,          # Number of token types (typically 0 for regular text, 1 for special tokens)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-14T01:31:23.620097Z",
     "end_time": "2023-09-14T01:31:23.846094Z"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(128,), dtype=tf.int32)\n",
    "embedding_layer = Embedding(input_dim=albert_config.vocab_size, output_dim=albert_config.hidden_size)(input_layer)\n",
    "pooling_layer = GlobalAveragePooling1D()(embedding_layer)\n",
    "output_layer = Dense(units=len(label_encoder.classes_), activation='softmax')(pooling_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-14T01:31:25.428097Z",
     "end_time": "2023-09-14T01:31:25.443097Z"
    }
   },
   "outputs": [],
   "source": [
    "train_input_ids = np.array(train_df['input_ids'].to_list())\n",
    "val_input_ids = np.array(val_df['input_ids'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-14T01:31:26.061715Z",
     "end_time": "2023-09-14T01:37:59.858466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "104/104 [==============================] - 31s 293ms/step - loss: 1.6981 - accuracy: 0.4373 - val_loss: 1.6059 - val_accuracy: 0.4337\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 34s 323ms/step - loss: 1.5211 - accuracy: 0.4376 - val_loss: 1.4458 - val_accuracy: 0.4337\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 42s 401ms/step - loss: 1.3792 - accuracy: 0.4376 - val_loss: 1.3332 - val_accuracy: 0.4337\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 39s 377ms/step - loss: 1.2918 - accuracy: 0.4376 - val_loss: 1.2745 - val_accuracy: 0.4337\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 38s 366ms/step - loss: 1.2484 - accuracy: 0.4376 - val_loss: 1.2471 - val_accuracy: 0.4337\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 37s 358ms/step - loss: 1.2271 - accuracy: 0.4376 - val_loss: 1.2336 - val_accuracy: 0.4337\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 45s 436ms/step - loss: 1.2153 - accuracy: 0.4376 - val_loss: 1.2254 - val_accuracy: 0.4337\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 47s 453ms/step - loss: 1.2072 - accuracy: 0.4376 - val_loss: 1.2189 - val_accuracy: 0.4334\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 41s 398ms/step - loss: 1.2006 - accuracy: 0.4375 - val_loss: 1.2132 - val_accuracy: 0.4331\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 39s 374ms/step - loss: 1.1946 - accuracy: 0.4376 - val_loss: 1.2075 - val_accuracy: 0.4334\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_input_ids, train_df['label_encoded'].values,\n",
    "    validation_data=(val_input_ids, val_df['label_encoded'].values),\n",
    "    epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:51:29.840527Z",
     "start_time": "2023-09-01T11:51:29.802528Z"
    }
   },
   "outputs": [],
   "source": [
    "test_input_ids = np.array(test_df['input_ids'].to_list())\n",
    "test_loss, test_accuracy = model.evaluate(test_input_ids, test_df['label_encoded'].values)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_df['input_ids'].to_list(), test_df['label_encoded'].values)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\n",
    "encoded_text = tokenize_text(sample_text)\n",
    "encoded_text = np.array([encoded_text]) \n",
    "predicted_class = model.predict(encoded_text)[0]\n",
    "predicted_sentiment = label_encoder.inverse_transform([predicted_class.argmax()])[0]\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
