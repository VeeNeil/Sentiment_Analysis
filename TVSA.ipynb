{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:51:27.586208Z",
     "start_time": "2023-09-01T11:51:27.569207Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-13 21:05:07.257725: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-13 21:05:08.795730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import  AlbertConfig, AlbertModel\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AlbertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:51:27.657209Z",
     "start_time": "2023-09-01T11:51:27.579209Z"
    }
   },
   "outputs": [],
   "source": [
    "Dataset_file = \"TV.csv\"\n",
    "df = pd.read_csv(Dataset_file)\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['Cảm xúc'])\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text, add_special_tokens=True, max_length=128, padding='max_length', truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['input_ids'] = train_df['Nhận xét đánh giá'].apply(tokenize_text)\n",
    "val_df['input_ids'] = val_df['Nhận xét đánh giá'].apply(tokenize_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:51:29.783527Z",
     "start_time": "2023-09-01T11:51:27.685210Z"
    }
   },
   "outputs": [],
   "source": [
    "albert_config = AlbertConfig(\n",
    "    vocab_size=30000,            # Size of the vocabulary\n",
    "    hidden_size=512,            # Size of the hidden layers\n",
    "    num_hidden_layers=8,        # Number of hidden layers\n",
    "    num_attention_heads=10,      # Number of attention heads\n",
    "    intermediate_size=1024,     # Size of the intermediate (feed-forward) layers\n",
    "    hidden_dropout_prob=0.2,    # Dropout probability for hidden layers\n",
    "    attention_probs_dropout_prob=0.2,  # Dropout probability for attention scores\n",
    "    max_position_embeddings=128,  # Maximum position embeddings (adjust based on your sequence length)\n",
    "    type_vocab_size=2,          # Number of token types (typically 0 for regular text, 1 for special tokens)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 21:10:38.804164: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 61440000 exceeds 10% of free system memory.\n",
      "2023-09-13 21:10:38.864518: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 61440000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(128,), dtype=tf.int32)\n",
    "embedding_layer = Embedding(input_dim=albert_config.vocab_size, output_dim=albert_config.hidden_size)(input_layer)\n",
    "pooling_layer = GlobalAveragePooling1D()(embedding_layer)\n",
    "output_layer = Dense(units=len(label_encoder.classes_), activation='softmax')(pooling_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:51:29.828526Z",
     "start_time": "2023-09-01T11:51:29.785537Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/neil/Documents/SA/Sentiment_Analysis/TVSA.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/neil/Documents/SA/Sentiment_Analysis/TVSA.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_input_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(train_df[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list())\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/neil/Documents/SA/Sentiment_Analysis/TVSA.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m val_input_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(test_df[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_input_ids = np.array(train_df['input_ids'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "415/415 [==============================] - 74s 156ms/step - loss: 1.5593 - accuracy: 0.4118 - val_loss: 1.3704 - val_accuracy: 0.4337\n",
      "Epoch 2/10\n",
      "415/415 [==============================] - 63s 152ms/step - loss: 1.2796 - accuracy: 0.4376 - val_loss: 1.2464 - val_accuracy: 0.4337\n",
      "Epoch 3/10\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 1.2200 - accuracy: 0.4376 - val_loss: 1.2246 - val_accuracy: 0.4337\n",
      "Epoch 4/10\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 1.2037 - accuracy: 0.4376 - val_loss: 1.2134 - val_accuracy: 0.4334\n",
      "Epoch 5/10\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 1.1922 - accuracy: 0.4375 - val_loss: 1.2028 - val_accuracy: 0.4331\n",
      "Epoch 6/10\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 1.1808 - accuracy: 0.4382 - val_loss: 1.1912 - val_accuracy: 0.4334\n",
      "Epoch 7/10\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 1.1687 - accuracy: 0.4403 - val_loss: 1.1784 - val_accuracy: 0.4389\n",
      "Epoch 8/10\n",
      "415/415 [==============================] - 62s 150ms/step - loss: 1.1557 - accuracy: 0.4476 - val_loss: 1.1653 - val_accuracy: 0.4461\n",
      "Epoch 9/10\n",
      "415/415 [==============================] - 62s 150ms/step - loss: 1.1419 - accuracy: 0.4626 - val_loss: 1.1514 - val_accuracy: 0.4714\n",
      "Epoch 10/10\n",
      "415/415 [==============================] - 65s 156ms/step - loss: 1.1278 - accuracy: 0.4871 - val_loss: 1.1373 - val_accuracy: 0.4741\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_input_ids, train_df['label_encoded'].values,\n",
    "    validation_data=(val_input_ids, val_df['label_encoded'].values),\n",
    "    epochs=10, batch_size=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:51:29.840527Z",
     "start_time": "2023-09-01T11:51:29.802528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 3ms/step - loss: 1.1373 - accuracy: 0.4741\n",
      "Test Loss: 1.1373, Test Accuracy: 0.4741\n"
     ]
    }
   ],
   "source": [
    "test_input_ids = np.array(test_df['input_ids'].to_list())\n",
    "test_loss, test_accuracy = model.evaluate(test_input_ids, test_df['label_encoded'].values)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_df['input_ids'].to_list(), test_df['label_encoded'].values)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 102ms/step\n",
      "Predicted Sentiment: tốt\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\n",
    "encoded_text = tokenize_text(sample_text)\n",
    "encoded_text = np.array([encoded_text]) \n",
    "predicted_class = model.predict(encoded_text)[0]\n",
    "predicted_sentiment = label_encoder.inverse_transform([predicted_class.argmax()])[0]\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
