{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-18T18:55:19.699688Z",
     "end_time": "2023-09-18T18:55:19.720681Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import  AlbertConfig, AlbertModel\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AlbertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-18T18:55:20.775692Z",
     "end_time": "2023-09-18T18:55:20.867685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training data from \"train.csv\"\n",
    "train_df = pd.read_csv(\"synthetic_train.csv\")\n",
    "\n",
    "# Load validation data from \"val.csv\"\n",
    "val_df = pd.read_csv(\"synthetic_val.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label_encoded'] = label_encoder.fit_transform(train_df['sentiment'])\n",
    "val_df['label_encoded'] = label_encoder.transform(val_df['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-18T18:55:21.763686Z",
     "end_time": "2023-09-18T18:55:22.584528Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text, add_special_tokens=True, max_length=128, padding='max_length', truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-18T18:55:23.390530Z",
     "end_time": "2023-09-18T18:55:27.951621Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['input_ids'] = train_df['sentence'].apply(tokenize_text)\n",
    "val_df['input_ids'] = val_df['sentence'].apply(tokenize_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-18T18:55:27.946622Z",
     "end_time": "2023-09-18T18:55:27.983621Z"
    }
   },
   "outputs": [],
   "source": [
    "albert_config = AlbertConfig(\n",
    "    vocab_size=30000,            # Size of the vocabulary\n",
    "    hidden_size=512,            # Size of the hidden layers\n",
    "    num_hidden_layers=10,        # Number of hidden layers\n",
    "    num_attention_heads=10,      # Number of attention heads\n",
    "    intermediate_size=1024,     # Size of the intermediate (feed-forward) layers\n",
    "    hidden_dropout_prob=0.2,    # Dropout probability for hidden layers\n",
    "    attention_probs_dropout_prob=0.2,  # Dropout probability for attention scores\n",
    "    max_position_embeddings=128,  # Maximum position embeddings (adjust based on your sequence length)\n",
    "    type_vocab_size=2,          # Number of token types (typically 0 for regular text, 1 for special tokens)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-18T18:55:31.152239Z",
     "end_time": "2023-09-18T18:55:31.963237Z"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(128,), dtype=tf.int32)\n",
    "embedding_layer = Embedding(input_dim=albert_config.vocab_size, output_dim=albert_config.hidden_size)(input_layer)\n",
    "pooling_layer = GlobalAveragePooling1D()(embedding_layer)\n",
    "output_layer = Dense(units=len(label_encoder.classes_), activation='softmax')(pooling_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-18T18:55:33.448801Z",
     "end_time": "2023-09-18T18:55:33.563802Z"
    }
   },
   "outputs": [],
   "source": [
    "train_input_ids = np.array(train_df['input_ids'].to_list())\n",
    "val_input_ids = np.array(val_df['input_ids'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-18T18:55:35.470262Z",
     "end_time": "2023-09-18T18:59:35.941849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 27s 404ms/step - loss: 1.0970 - accuracy: 0.4284 - val_loss: 1.0950 - val_accuracy: 0.4479\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 28s 437ms/step - loss: 1.0930 - accuracy: 0.4206 - val_loss: 1.0912 - val_accuracy: 0.4764\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 25s 392ms/step - loss: 1.0889 - accuracy: 0.5259 - val_loss: 1.0872 - val_accuracy: 0.5688\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 25s 388ms/step - loss: 1.0845 - accuracy: 0.5263 - val_loss: 1.0828 - val_accuracy: 0.5580\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 23s 359ms/step - loss: 1.0796 - accuracy: 0.6054 - val_loss: 1.0778 - val_accuracy: 0.6218\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 24s 380ms/step - loss: 1.0742 - accuracy: 0.5906 - val_loss: 1.0722 - val_accuracy: 0.6090\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 24s 382ms/step - loss: 1.0680 - accuracy: 0.6586 - val_loss: 1.0660 - val_accuracy: 0.6341\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 22s 340ms/step - loss: 1.0612 - accuracy: 0.6622 - val_loss: 1.0591 - val_accuracy: 0.6262\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 21s 328ms/step - loss: 1.0536 - accuracy: 0.6594 - val_loss: 1.0516 - val_accuracy: 0.6518\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 21s 323ms/step - loss: 1.0455 - accuracy: 0.6583 - val_loss: 1.0434 - val_accuracy: 0.6488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    train_input_ids, train_df['label_encoded'].values,\n",
    "    validation_data=(val_input_ids, val_df['label_encoded'].values),\n",
    "    epochs=10, batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-18T19:00:23.878293Z",
     "end_time": "2023-09-18T19:00:24.512295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 8ms/step - loss: 1.0434 - accuracy: 0.6488\n",
      "Test Loss: 1.0434, Test Accuracy: 0.6488\n"
     ]
    }
   ],
   "source": [
    "test_input_ids = np.array(val_df['input_ids'].to_list())\n",
    "test_loss, test_accuracy = model.evaluate(test_input_ids, val_df['label_encoded'].values)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_df['input_ids'].to_list(), test_df['label_encoded'].values)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\n",
    "encoded_text = tokenize_text(sample_text)\n",
    "encoded_text = np.array([encoded_text]) \n",
    "predicted_class = model.predict(encoded_text)[0]\n",
    "predicted_sentiment = label_encoder.inverse_transform([predicted_class.argmax()])[0]\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
